{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d552e586-38c2-4819-abf0-ca31a3e11024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "from simplify_docx import simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8cc72c-7e5b-48b2-b838-d97f96243a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Analysis_12_30_21_Colorado_Fire_segments.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d384bf03-b360-43f9-a1a5-51cc2923565b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    doc = docx.Document(filename)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def docx_to_clean_dict(docx_as_json, first_table_index=1):\n",
    "  \"\"\"Takes docx_as_json and cleans it up\n",
    "  return: list of dicts\n",
    "    {\n",
    "      \"time\": ___,\n",
    "      \"location\": ____,\n",
    "      \"station\": ____,\n",
    "      \"text\": _____________\n",
    "    }\n",
    "  \"\"\"\n",
    "  clean_data = []\n",
    "\n",
    "  for blob in docx_as_json['VALUE'][0]['VALUE'][first_table_index:]:\n",
    "      text_end = False\n",
    "\n",
    "      if blob['TYPE'] == 'table':\n",
    "          time = blob['VALUE'][0]['VALUE'][0]['VALUE'][0]['VALUE'][0]['VALUE']\n",
    "          location = blob['VALUE'][0]['VALUE'][1]['VALUE'][0]['VALUE'][0]['VALUE']\n",
    "          station = blob['VALUE'][0]['VALUE'][2]['VALUE'][0]['VALUE'][0]['VALUE']\n",
    "\n",
    "      if blob['TYPE'] == 'paragraph':\n",
    "          text = blob['VALUE'][0]['VALUE']\n",
    "          text_end = True\n",
    "\n",
    "      if text_end:\n",
    "          clean_data.append({\n",
    "              \"time\": time,\n",
    "              \"location\": location,\n",
    "              \"station\": station,\n",
    "              \"text\": text\n",
    "          })\n",
    "\n",
    "  return clean_data\n",
    "\n",
    "\n",
    "\n",
    "def read_docx_to_dict(filename):\n",
    "    \"\"\"Reads in docx file and converts it to a list of dicts\"\"\"\n",
    "  # read in a document\n",
    "  doc = docx.Document(filename)\n",
    "\n",
    "  # coerce to JSON using the standard options\n",
    "  docx_as_json = simplify(doc)\n",
    "\n",
    "  blob_types = [blob['TYPE'] for blob in docx_as_json['VALUE'][0]['VALUE']]\n",
    "\n",
    "  first_table_index = blob_types.index('table')\n",
    "\n",
    "  return docx_to_clean_dict(docx_as_json, first_table_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868fffd-a0ed-4244-933d-bf1fc8756b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_docx_to_dict(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb858c-b020-4ad7-b81f-b757a56f9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 500\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03cddec-d58c-4a51-b70f-ec97bddc11ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad8898-5d43-490e-86b8-23ee2dbe2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f58b2-a437-4abe-84f2-0d717dd7485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from thefuzz import fuzz\n",
    "\n",
    "def check_text_likeness(df, text, ratio=85, row_name='text'):\n",
    "    \"\"\"For a given dataframe (df), loop through the column (row_name),\n",
    "    calculate the partial ratio between given text (text) and the text in each row,\n",
    "    and return the indexes where the partial ratio is greater than or equal to the ratio\n",
    "    \"\"\"\n",
    "    matches = df.apply(lambda row: (fuzz.partial_ratio(row[row_name], text) >= ratio), axis=1)\n",
    "    return [i for i, x in enumerate(matches) if x]\n",
    "\n",
    "# def extract_similar_texts(df, text, ratio=85, row_name='text'):\n",
    "#     start = time.time()\n",
    "#     start_time = time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "#     print(f'start time: {start_time}')\n",
    "\n",
    "#     check_text_likeness(df, text, ratio=85, row_name='text')\n",
    "\n",
    "#     end = time.time()\n",
    "#     minutes = (end - start)/60.0\n",
    "#     end_time = time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "#     print(f'end time: {end_time} -- took {minutes} minutes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529cad2-6033-4bd7-a281-4039bcdcc863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['matches'] = df.apply(lambda row: check_text_likeness(df, row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af4e2e-0658-480b-bbec-51e6b26bb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_biggest_text(idx_list):\n",
    "    \"\"\"For the rows with similar text, fetch the biggest text's index\"\"\"\n",
    "    biggest_length = 0\n",
    "    idx = None\n",
    "\n",
    "    if len(idx_list) == 1:\n",
    "        return idx_list[0]\n",
    "\n",
    "    for i in idx_list:\n",
    "        current_length = len(df['text'][i])\n",
    "        if current_length > biggest_length:\n",
    "            biggest_length = current_length\n",
    "            idx = i\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acde08-ca8c-487c-9ccd-dd73da946c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['row_to_use'] = df.apply(lambda row: fetch_biggest_text(row['matches']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60950ce-6f9a-4b22-bd8b-d759f71259f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_use_row():\n",
    "  # mark rows to use\n",
    "  idxs = list(df['row_to_use'].unique())\n",
    "\n",
    "  for index, row in df.iterrows():\n",
    "      df.at[index,'use_row'] = index in idxs\n",
    "\n",
    "  return 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf96813-4283-40e3-9c95-469d7f018f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_use_row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96a769-f143-46ef-baf6-ea7956f56a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512af26-564a-44a1-b113-751b8b9333d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words'] = df['text'].str.lower().str.replace(',', '').str.replace('>', '').str.replace('.', '').str.replace('\\n', '').str.replace('â€™', \"'\").str.replace(\n",
    "    '!', '').str.replace('?', '').str.replace('%', '').str.replace(')', '').str.replace('(', '').str.replace('_', '').str.replace(':', '').str.strip().str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895de87-db1c-4f10-ba2a-05d56cb87e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87bc4b2-3175-4a85-8c89-2a82528920a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f96f0-a776-4778-ab3e-6b29b8f74905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import parse_words\n",
    "df['clean_words'] = df.apply(lambda row: parse_words(row['words']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b69ca-4e76-4ad9-96c7-b4f4c5afc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47592d97-fa56-4662-a032-3cb479c49bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135baa9-c3c3-44a4-a75a-9103abd0a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import fetch_climate_words_in_words, fetch_climate_phrases_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43d3b8-225f-4f29-83d7-e34ef61b3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_climate_words_in_words([\"adapt\",\"for\", \"climate\", \"change\"])\n",
    "# segment_df['climate_words_found'] = segment_df.apply(lambda row: fetch_climate_words_in_text(row['clean_words']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d738399-d5bd-4bc8-adfb-5f4f9acbe6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_climate_phrases_in_text(\"adapt for climate change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d6518-7358-4697-b698-fa0e7311c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['climate_words_found'] = df.apply(lambda row: fetch_climate_words_in_words(row['clean_words']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9326a4-053f-4f6d-837c-865a3b7ab775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ba0fd-2f49-4add-9487-373939b080e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['climate_phrases_found'] = df.apply(lambda row: fetch_climate_phrases_in_text(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95ff39-a630-476a-b2ec-16dff7f41703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee4da9-a149-4cd9-88e2-dc6cd8479e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to csv\n",
    "df.to_csv('reports/abc_all.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065656c-6bf3-4961-8a94-6eb04201a303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683e91f-1361-4af6-9bd3-e356b4be1ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00113b70-1aad-45c7-adb7-b17fe6a6cec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0a0fd-20f7-4639-ba71-7c38f4b8471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df = df[df['use_row']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6749a-fc15-4377-852b-ca9066a9f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee88f7a-3d83-40ef-999b-249bfd9377d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = unique_df['clean_words'].str.len().sum()\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737b741-4e5d-4ef5-a943-60cf99384f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_found_master_list(df_clean_words):\n",
    "    \"\"\"Given a column of words, aggregate master list\"\"\"\n",
    "    words_found = list()\n",
    "    for chunk in df_clean_words:\n",
    "        words_found += chunk\n",
    "\n",
    "    return words_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c30a84-8856-4e96-81d8-f0d33ac201c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_found = words_found_master_list(unique_df['clean_words'])\n",
    "len(words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78c8aa-06e5-4686-a263-f3ef077996d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "def master_stopwords_list():\n",
    "    \"\"\"Creates a master list of stopwords from pre-existing stopwords found in nltk and wordcloud\"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    final_stopwords = list(STOPWORDS) + list(stop_words)\n",
    "    return [i.lower() for i in set(final_stopwords)]\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Given a list of words, distill to root words\"\"\"\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    lemma_list = []\n",
    "    for word, tag in pos_tag(words):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = lem.lemmatize(word, pos=wntag)\n",
    "        lemma_list.append(lemma)\n",
    "    return lemma_list\n",
    "\n",
    "def clean_lemmatized_words(lemma_words):\n",
    "    \"\"\"Removes stop words from the lemma list\"\"\"\n",
    "    nonstop_lemma_words = []\n",
    "    final_stopwords = master_stopwords_list()\n",
    "\n",
    "    for word in lemma_words:\n",
    "        if word not in final_stopwords:\n",
    "            nonstop_lemma_words.append(word)\n",
    "\n",
    "    return list(filter(None, nonstop_lemma_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a987f-7ca9-4f50-8217-9e7c69540ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lemma_words = clean_lemmatized_words(lemmatize_words(words_found))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8632540d-1bb5-405a-b69b-7936707790b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "lfdist = FreqDist(clean_lemma_words)\n",
    "lfdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb54c1d-3da4-4aed-9871-cd3208beaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lfdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a19ad8-9f5f-42cf-84eb-a7272f225b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from wordcloud import STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = master_stopwords_list()).generate_from_frequencies(lfdist)\n",
    "\n",
    "# Plot\n",
    "plt.figure( figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('word_cloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2313ec1-ce01-4dd0-98f8-fcec5c23cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 500\n",
    "words_df = pd.DataFrame(lfdist.items(), columns=['Word', 'Count'])\n",
    "\n",
    "words_df.sort_values(by=['Count'], ascending=False, inplace=True)\n",
    "len(words_df)\n",
    "# 1374 total words\n",
    "\n",
    "words_df['Count'].sum()\n",
    "\n",
    "# create data\n",
    "climate_change_words_df = words_df.loc[words_df['Word'].isin(CLIMATE_CHANGE_RELATED_WORDS)]\n",
    "\n",
    "climate_words_count = climate_change_words_df['Count'].sum()\n",
    "non_climate_words_count = words_df['Count'].sum() - climate_words_count\n",
    "\n",
    "comparison_df = pd.DataFrame({'Words': ['Climate-related', 'Non Climate-related'],\n",
    "                             'counts': [climate_words_count, non_climate_words_count]})\n",
    "comparison_df.set_index('Words', inplace=True)\n",
    "print(comparison_df)\n",
    "\n",
    "plot = comparison_df.plot.pie(y='counts', title=\"Climated-related vs non climated-related word frequencies\", legend=True, autopct='%1.1f%%', shadow=True, figsize=(8, 8))\n",
    "\n",
    "fig = plot.get_figure()\n",
    "#fig.savefig(\"comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94a333-ccac-4388-932b-db04ac59d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find climate related word frequencies\n",
    "\n",
    "# set figure size\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# plot horizontal bar plot\n",
    "climate_change_words_df.sort_values(by='Count').plot.barh(x=\"Word\", y=\"Count\", ax=ax)\n",
    "# set the title\n",
    "plt.title(\"Count of climate change related words\")\n",
    "\n",
    "for i, v in enumerate(climate_change_words_df['Count'].sort_values()):\n",
    "    ax.text(v, i , str(v),\n",
    "            color = 'blue', fontweight = 'bold')\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig('climate-related-words-breakdown.png', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840975d-5190-45be-ad5e-53b02ff0b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find segments\n",
    "climate_change_words_found = list(climate_change_words_df['Word'].unique())\n",
    "climate_change_words_found\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765949a-c2ee-4a87-a608-39c0646caf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df[unique_df[\"climate_words_found\"].str.len() != 0].to_csv('reports/abc_final.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44a3a7-b324-4e96-9a81-851e38405c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b92856b-9496-41d4-89a3-f4abcf8f05f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
